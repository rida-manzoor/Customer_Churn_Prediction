# Install dependencies (for Colab)
!pip install imbalanced-learn shap xgboost

# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

from imblearn.over_sampling import SMOTE
import shap

# ==========================
# Step 1: Load Dataset
# ==========================

# Example: Telecom churn dataset from Kaggle
url = "https://raw.githubusercontent.com/blastchar/telco-customer-churn/master/WA_Fn-UseC_-Telco-Customer-Churn.csv"
data = pd.read_csv(url)

print("Dataset Shape:", data.shape)
data.head()

# ==========================
# Step 2: Data Preprocessing
# ==========================

# Drop customerID (not useful for prediction)
data.drop('customerID', axis=1, inplace=True)

# Convert TotalCharges to numeric
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')
data['TotalCharges'].fillna(data['TotalCharges'].median(), inplace=True)

# Encode categorical variables
from sklearn.preprocessing import LabelEncoder

for col in data.select_dtypes(include=['object']).columns:
    if col != 'Churn':
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])

# Encode target
data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})

# Train-test split
X = data.drop('Churn', axis=1)
y = data['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Handle class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Feature scaling
scaler = StandardScaler()
X_train_res = scaler.fit_transform(X_train_res)
X_test = scaler.transform(X_test)

# ==========================
# Step 3: Exploratory Data Analysis (EDA)
# ==========================

plt.figure(figsize=(6,4))
sns.countplot(x='Churn', data=data)
plt.title('Churn Distribution')
plt.show()

plt.figure(figsize=(10,6))
sns.heatmap(data.corr(), cmap='coolwarm', annot=False)
plt.title('Feature Correlation Heatmap')
plt.show()

# ==========================
# Step 4: Model Training
# ==========================

# Logistic Regression
log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train_res, y_train_res)

# Random Forest
rf_model = RandomForestClassifier(n_estimators=200, random_state=42)
rf_model.fit(X_train_res, y_train_res)

# XGBoost
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train_res, y_train_res)

# ==========================
# Step 5: Evaluation
# ==========================

def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.show()
    print("ROC-AUC:", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))

print("\nLogistic Regression Results:")
evaluate_model(log_model, X_test, y_test)

print("\nRandom Forest Results:")
evaluate_model(rf_model, X_test, y_test)

print("\nXGBoost Results:")
evaluate_model(xgb_model, X_test, y_test)

# ==========================
# Step 6: Explainability with SHAP
# ==========================

explainer = shap.Explainer(rf_model, X_train_res)
shap_values = explainer(X_test[:100])

shap.summary_plot(shap_values, X_test[:100], feature_names=X.columns)

